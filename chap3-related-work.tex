%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Related Work and Other Approaches to Parsing} \label{sec:related}

  Stepping back a bit, we describe how our approach to parsing relates to existing work.
  
  \todo{Mention LR (and LL(1)?) parsers: ``The field of parsing is one of the most venerable in computer-science.  Still with us are a variety of parsing approaches born in times of much more severe constraints on memory and processor speed, including various flavors of LR parsers, which apply only to strict subsets of the context-free grammars, to guarantee ability to predict which production applies based on finite look-ahead into a string.''}
  
  \section{Recursive Descent Parsing}
    The most conceptually straightforward approaches to parsing fall into the class called recursive descent parsing, where, to parse a string $s$ as a given production $p$, you attempt to parse various parts of $s$ as each of the items in the list $p$.  The control flow of the code mirrors the structure of the grammar, as well as the structure of the eventual parse tree, descending down the branches of the parse tree, recursively calling itself at each step.
    
    \subsection{Parser Combinators}
      A popular approach, called \emph{combinator parsing}~\cite{pcomb}, to implementing recursive descent parsing involves writing a small set of typed combinators, or higher-order functions, which are then applied to each other in various combinations to write a parser that mimics closely the structure of the grammar.
      
      Essentially, parsers defined via parser combinators answer the question ``what prefixes of a given string be parsed as a given item?''  Each function returns a list of postfixes of the string they are passed, indicating all of the strings that might remain for the other items in a given rule.  \todoask{Should I mention semantic actions?}
      
    \subsubsection{Basic Combinators}
      We now define the basic combinators.  In the simplest form, each combinator takes in a string, and returns a list of strings (the postfixes); we can define the type \fname{parser} as \indname{String}~$\to$~\typelist{\indname{String}}.  We can define the empty-string parser, as well as the parser for a nonterminal with no production rules, which always fails:
\begin{align*}
&\epsilon~:~\fname{parser}\\
&\epsilon~\farg{str}~=~\valuelist{\farg{str}} \\ \\
&\fname{fail}~:~\fname{parser}\\
&\fname{fail}~\termhole~=~\nil
\end{align*}
      Failure is indicated by returning the empty list; success at parsing the entire string is indicated by returning a list containing the empty string.
      
      The parser for a given terminal fails if the string does not start with that character, and returns all but the first character if it does:
\begin{align*}
&\fname{terminal}~:~\indname{Char}~\to~\fname{parser}\\
&\fname{terminal}~\farg{ch}~(\cons{\farg{ch}}{\farg{str}})~=~\valuelist{\farg{str}} \\
&\fname{terminal}~\aswidthof{\termhole}{\farg{ch}}~\str{}~=~\nil
\end{align*}
      We now define combinators for sequencing and alternatives:
\newcommand{\sequencing}{\fname{>{}>{}>}}
\begin{align*}
&(\sequencing)~:~\fname{parser}~\to~\fname{parser}~\to~\fname{parser}\\
&(\farg{p\ensuremath{_0}}~\sequencing~\farg{p\ensuremath{_1}})~\farg{str}~=~\fname{flat\_map}~\farg{p\ensuremath{_1}}~(\farg{p\ensuremath{_0}}~\farg{str}) \\ \\
&(\fname{|||})~:~\fname{parser}~\to~\fname{parser}~\to~\fname{parser}\\
&(\farg{p\ensuremath{_0}}~\fname{|||}~\farg{p\ensuremath{_1}})~\farg{str}~=~\farg{p\ensuremath{_0}}~\farg{str}~\fname{++}~\farg{p\ensuremath{_1}}~\farg{str}
\end{align*}
      where \fname{++} is list concatenation, and \fname{flat\_map}, which concatenates the lists returned by mapping its first argument over each of the elements in its second argument, has type \texttt{(\farg{A} $\to$ \typelist{\farg{B}}) $\to$ \typelist{\farg{A}} $\to$ \typelist{\farg{B}}}, where \typelist{\farg{A}} is the type denoting a list of elements, all of type \farg{A}.\todo{Check if I've defined this notation already.}
      
    \subsubsection{An Example}
    
      We can now easily define a parser for the grammar \regex{(ab)$^*$}:
\begin{align*}
&\fname{parse-\regex{(ab)\ensuremath{^*}}}~:~\fname{parser} \\
&\fname{parse-\regex{(ab)\ensuremath{^*}}}~=~(\fname{terminal}~\terminal{a}~\sequencing~\fname{terminal}~\terminal{b}~\sequencing~\fname{parse-\regex{(ab)\ensuremath{^*}}})~~\fname{|||}~~\epsilon
\end{align*}
      Note that, by putting $\epsilon$ last, we ensure that this parser returns the list in order of longest parse (shortest postfix) to shortest parse (longest postfix).
      
    \subsubsection{Proving Correctness and Dealing with Nontermination}
      Although parser combinators are straightforward, it is easy to make them loop forever.  It is well-known that parsers defined naively using parser combinators don't handle grammars with \emph{left recursion}, where the first item in a given production rule is the nonterminal currently being defined.  For example, if we have the nonterminal \nt{expr} $\Coloneqq$ \nt{number}~|~\nt{expr}~\terminal{+}~\nt{expr}, then the parser for \nt{expr}~\terminal{+}~\nt{expr} will call the parser for \nt{expr}, which will call the parse for \nt{expr}~\terminal{+}~\nt{expr}, which will quickly loop forever.
      
      The algorithm we presented in \autoref{sec:solve-nontermination} is essentially the same as the algorithm Ridge presents in~\cite{Ridge} to deal with this problem.  By wrapping the calls to the parsers, in each combinator, with a function that prunes duplicative calls, Ridge provides a way to ensure that parsers terminate.  Also included in \cite{Ridge} are proofs in HOL4 that such wrapped parsers are both sound (and therefore terminating) and complete.  Furthermore, Ridge's parser has worst-case $O(n^5)$ running time in the input-string length.

  \subsection{Parsing with Derivatives}
    \Citeauthor{Derivs} describe an elegant method for recursive descent parsing in~\cite{Derivs}, based on Brzozowski's derivatives~\cite{BrzozowskiDerivs}, which might be considered a conceptual dual to standard combinator parser.  Rather than returning a list of possible string remnants, constructed by recursing down the structure of the grammar, we can iterate down the characters of a string, computing an updated language, or grammar, at each point.
    
    The \emph{language} defined by a grammar is a the set of strings accepted by that grammar.  Here we describe the mathematical ideas behind parsing with derivatives.  \citeauthor*{Derivs} take a slightly different approach to ensure termination; where we will describe the mathematical operations on languages, they define these operations on a structural representation of the language, akin to an inductive definition of the grammar.
    
    Much as we defined a parser combinators for the elementary operations of a grammar ($\epsilon$, terminals, sequencing, and alternatives), we can define similar combinators for defining a (lazy, or coinductive) language for a grammar.  Defining the type \fname{language} to be a set (or just a coinductive list) of strings, we have:
\begin{align*}
& \epsilon~:~\fname{language} \\
& \epsilon~=~\{\str{}\} \\ \\
& \fname{terminal}~:~\indname{Char}~\to~\fname{language} \\
& \fname{terminal}~\farg{ch}~=~\{\farg{ch}\} \\ \\
& (\fname{\sequencing})~:~\fname{language}~\to~\fname{language}~\to~\fname{language} \\
& \mathcal{L}_0~\fname{\sequencing}~\mathcal{L}_1~=~\{~\strcat{\farg{s\ensuremath{_0}}}{\farg{s\ensuremath{_1}}}~|~\farg{s\ensuremath{_0}}\in\mathcal L_0\text{ and }\farg{s\ensuremath{_1}}\in\mathcal{L}_1 \} \\ \\
& (\fname{|||})~:~\fname{language}~\to~\fname{language}~\to~\fname{language} \\
& \mathcal{L}_0~\fname{|||}~\mathcal{L}_1~=~\mathcal L_0\cup\mathcal L_1
\end{align*}

    The essential operations for computing derivatives are \emph{filtering} and \emph{chopping}.  To \emph{filter} a language $\mathcal L$ by a character $c$ is to take the subset of strings in $\mathcal L$ which start with $c$.  To chop a language $\mathcal L$ is to remove the first character from every string.  The derivative $D_c(\mathcal L)$ with respect to $c$ of a language $\mathcal L$ is then the language $\mathcal L$, filtered by $c$ and chopped:
\begin{align*}
&\fname{D}_{\farg{c}}~:~\fname{language}~\to~\fname{language} \\
&\fname{D}_{\farg{c}}~\emptyset~=~\emptyset \\
&\fname{D}_{\farg{c}}~(\cons{(\cons{\farg{ch}}{\farg{str}})}{\mathcal L})~=~\cons{\farg{str}}{(\fname{D}_{\farg{c}}~\mathcal{L})} \\
&\fname{D}_{\farg{c}}~(\cons{\termhole}{\mathcal L})~=~\fname{D}_{\farg{c}}~\mathcal{L}
\end{align*}

      We can then define a \fname{has\_parse} proposition by taking successive derivatives:
\begin{align*}
&\fname{has\_parse}~:~\coqtype{language}~\to~\coqtype{String}~\to~\Prop \\
&\fname{has\_parse}~\mathcal{L}~\str{}~=~\str{}\in\mathcal{L} \\
&\fname{has\_parse}~\mathcal{L}~(\cons{\farg{ch}}{\farg{str}})~=~\fname{has\_parse}~(\fname{D\ensuremath{_{\farg{ch}}}}~\mathcal L)~\farg{str} \\
\end{align*}
      
      To ensure termination and good performance, \citeauthor*{Derivs} define the derivative operation on the structure of the grammar, rather than defining combinators that turn a grammar into a language, and furthermore take advantage of laziness and memoization.  After adding code to prune the resulting language of useless content, they argue that the cost of parsing with derivatives is $\mathcal O(n|G|)$ on average, where $n$ is the length of the input string and $|G|$ is the size of the grammar.
      
    \subsubsection{Formal Verification}
      \Citeauthor{DerivsCoq} formally verify, in Coq, finite automata for parsing the fragment of derivative-based parsing which applies to regular expressions.~\cite{DerivsCoq}  This fragment dates back to Brzozowski's original presentation of derivatives.~\cite{BrzozowskiDerivs}\todoask{Should I define ``finite automata?}
      
  
  \todo{Fill in content}
  \todoask{Am I missing any branches?}
  \begin{itemize}
    \item Say something about recursive descent parsing seeming obvious and trivial only after writing out the types inductively?
    \item Say something about LR parsers and processor/memory constraints?
    \item Parser combinators
      \begin{itemize}
        \item parse by consuming characters from the string.
        \item Allow incremental parsing
      \end{itemize}
    \item Look into what \cite{PEG}s and \cite{GLL}s are.
    \item Approaches to verifying parsers: correct-by-construction, induction.  Other way?  \todo{Read paper}
  \end{itemize}

  \todo{Rewrite the rest of this part so it's not just Adam's words}
  
    However, despite rumors to the contrary, the field of parsing is far from dead.  In the twentieth century, the functional-programming world experimented with a variety of approaches to \emph{parser combinators}~\cite{pcomb}, where parsers are higher-order functions built from a small set of typed combinators.  In the twenty-first century alone, a number of new parsing approaches have been proposed or popularized, including parsing expression grammars (PEGs)~\cite{PEG}, derivative-based parsing~\cite{Derivs}, and GLL parsers~\cite{GLL}.

  However, our approach is essentially the same, algorithmically, as the one that Ridge demonstrated with a verified parser-combinator system~\cite{Ridge}, taking naive recursive-descent parsing and adding a layer to prune duplicative calls to the parser.  His proof was carried out in HOL4, necessarily without using dependent types.  Our new work may be interesting for the aesthetic appeal of our unusual application of dependent types to get the parser to generate some of its own soundness proof.  Ridge's parser also has worst-case $O(n^5)$ running time in the input-string length.  In the context of our verified implementation, we plan to explore a variety of optimizations based on clever, grammar-specific choices of string-splitter functions, which should have a substantial impact on the run-time cost of parsing some relevant grammars, and which we conjecture will not require any changes to the development presented in this paper.

  A few other past projects have verified parsers with proof assistants, applying to derivative-based parsing~\cite{DerivsCoq} and SLR~\cite{SLR} and LR(1)~\cite{LR1} parsers.  Several projects have used proof assistants to apply verified parsers within larger programming-language tools.  RockSalt~\cite{RockSalt} does run-time memory-safety enforcement for x86 binaries, relying on a verified machine-code parser that applies derivative-based parsing for regular expressions.  The verified Jitawa~\cite{Jitawa} and CakeML~\cite{CakeML} language implementations include verified parsers, handling Lisp and ML languages, respectively.

  Our final parser derivation relies on a relational parametricity property for polymorphic functions in Coq's type theory Gallina.  With Coq as it is today, we need to prove this property manually for each eligible function, even though we can prove metatheoretically that it holds for them all.  Bernardy and Guilhem~\cite{InColor} have shown how to extend type theories with support for materializing ``free theorem'' parametricity facts internally, and we might be able to simplify our implementation using such a feature.
  \todo{Flesh this out, rewrite it so it's not all Adam's words, read the papers}

\section{What's New and What's Old} \label{sec:new} \label{sec:goals}
  The goal of this project is to demonstrate a new approach to generating parsers: incrementally building efficient parsers by refinement.
  
  We begin with naive recursive-descent parsing.\todo{Citation?}  \todo{Say something about viewing recursive-descent as an instance of general inhabitation-decision not being anywhere in the literature?}  We ensure termination via memoization, a la~\cite{Ridge}.  We parameterize the parser on a ``splitting oracle'', which describes how to recurse (\autoref{sec:splitting-oracle}).  As far as we can tell, the idea of factoring the algorithmic complexity like this is new.
  
  We use Fiat to incrementally build efficient parsers by refinement; we describe Fiat starting in \autoref{ch:fiat}.
  
  Additionally, we take a digression in \autoref{ch:dep-types} to describe how our parser can be used to prove its own completeness; the idea of reusing the parsing algorithm to generate proofs, parsing parse trees rather than strings, is not found in the literature, to the authors' knowledge.
