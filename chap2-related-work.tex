%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Related Work and Other Approaches to Parsing} \label{sec:related}

  Stepping back a bit, we describe how our approach to parsing relates to existing work.
  
  \section{Coq} \label{sec:coq}
    \editedtext[1]{As stated in \autoref{sec:first-short-coq-intro}, we define our parser and prove its correctness in the proof assistant Coq~\cite{Coq12}.  Like other proof assistants utilizing dependent type theory, Coq takes advantage of the Curry-Howard isomorphism~\cite{Curry,Howard} to allow proofs to be written as functional programs; dependent types allow universal and existential quantification.  Coq natively permits only structural recursion, where recursive function calls may be invoked only on direct structural subterms of a given, specified argument.  The standard library defines combinators for turning well-founded recursion into structural recursion, which can be used to define essentially all recursive functions which provably halt in all cases (which is a class containing, as it turns out, essentially all algorithms of interest).  Coq's mathematical language, Gallina, implements Martin-L\"of's dependent lambda calculus.  Coq has a separate tactic language, called \Ltac~\cite{delahaye2000tactic}, which allows imperative construction of proof objects, and functions, by forwards and backwards reasoning.}
  
  \section{Recursive-Descent Parsing}
    The most conceptually straightforward approaches to parsing fall into the class called recursive-descent parsing, where, to parse a string $s$ as a given production $p$, you attempt to parse various parts of $s$ as each of the items in the list $p$.  The control flow of the code mirrors the structure of the grammar, as well as the structure of the eventual parse tree, descending down the branches of the parse tree, recursively calling itself at each step.  \newtext[2]{The algorithm we have described in \autoref{ch:basic-rdp-algorithm} seems to fall out almost trivially from the inductive description of parse trees; we come back to this in \autoref{sec:generalized-inhabitation-decision} when we briefly sketch how it should be possible to generalize this algorithm to other inductive type families.}
    
    \subsection{Parser Combinators}
      A popular approach to implementing recursive-descent parsing, called \emph{combinator parsing}~\cite{pcomb}, involves writing a small set of typed combinators, or higher-order functions, which are then applied to each other in various combinations to write a parser that mimics closely the structure of the grammar.
      
      Essentially, parsers defined via parser combinators answer the question ``what prefixes of a given string can be parsed as a given item?''  Each function returns a list of postfixes of the string it is passed, indicating all of the strings that might remain for the other items in a given rule.
      
    \subsubsection{Basic Combinators}
      We now define the basic combinators.  In the simplest form, each combinator takes in a string, and returns a list of strings (the postfixes); we can define the type
\editedtext[1]{\begin{align*}
& \fname{parser}~\coloneqq~ \indname{String}~\to~\typelist{\indname{String}}.
\end{align*}}
     We can define the empty-string parser, as well as the parser for a nonterminal with no production rules, which always fails:
\begin{align*}
&\epsilon~:~\fname{parser}\\
&\epsilon~\farg{str} \functiondefeq \valuelist{\farg{str}} \\ \\
&\fname{fail}~:~\fname{parser}\\
&\fname{fail}~\termhole \functiondefeq \nil
\end{align*}
      Failure is indicated by returning the empty list; success at parsing the entire string is indicated by returning a list containing the empty string.
      
      The parser for a given terminal fails if the string does not start with that character, and returns all but the first character if it does:
\begin{align*}
&\fname{terminal}~:~\indname{Char}~\to~\fname{parser}\\
&\fname{terminal}~\farg{ch}~(\cons{\farg{ch}}{\farg{str}}) \functiondefeq \valuelist{\farg{str}} \\
&\fname{terminal}~\aswidthof{\termhole}{\farg{ch}}~\termhole \functiondefeq \nil
\end{align*}
      We now define combinators for sequencing and alternatives:
\newcommand{\sequencing}{\fname{>{}>{}>}}
\begin{align*}
&(\sequencing)~:~\fname{parser}~\to~\fname{parser}~\to~\fname{parser}\\
&(\farg{p\ensuremath{_0}}~\sequencing~\farg{p\ensuremath{_1}})~\farg{str} \functiondefeq \fname{flat\_map}~\farg{p\ensuremath{_1}}~(\farg{p\ensuremath{_0}}~\farg{str}) \\ \\
&(\fname{|||})~:~\fname{parser}~\to~\fname{parser}~\to~\fname{parser}\\
&(\farg{p\ensuremath{_0}}~\fname{|||}~\farg{p\ensuremath{_1}})~\farg{str} \functiondefeq \farg{p\ensuremath{_0}}~\farg{str}~\fname{++}~\farg{p\ensuremath{_1}}~\farg{str}
\end{align*}
      where \fname{++} is list concatenation, and \fname{flat\_map}, which concatenates the lists returned by mapping its first argument over each of the elements in its second argument, has type \texttt{(\farg{A} $\to$ \typelist{\farg{B}}) $\to$ \typelist{\farg{A}} $\to$ \typelist{\farg{B}}}.
      
    \subsubsection{An Example}
    
      We can now easily define a parser for the grammar \regex{(ab)$^*$}:
\begin{align*}
&\fname{parse\_\regex{(ab)\ensuremath{^*}}}~:~\fname{parser} \\
&\fname{parse\_\regex{(ab)\ensuremath{^*}}} \functiondefeq (\fname{terminal}~\terminal{a}~\sequencing~\fname{terminal}~\terminal{b}~\sequencing~\fname{parse\_\regex{(ab)\ensuremath{^*}}})~~\fname{|||}~~\epsilon
\end{align*}
      Note that, by putting $\epsilon$ last, we ensure that this parser returns the list in order of longest parse (shortest postfix) to shortest parse (longest postfix).
      
    \subsubsection{Semantic Actions}
      \newtext[1]{Frequently, programmers want parsers to not just say whether or not a given string, or prefix of a string, can be parsed, but to also build a parse tree, or perform some other computation or construction on the structure of the string.  A common way to accomplish this is with \emph{semantic actions}:  Associate to each production a function which, when given values associated to each of the nonterminals in its sequence, computes a value to associate to the given nonterminal.  By calling these functions at each node of the parse tree, passing the function at each node the values returned by its descendants, we can compute a value associated to a string as we parse it. For example, we might annotate a simple expression grammar, to compute the numerical value associated with a string expression, like this:}
\begin{align*}
  e \mathrel{\vcentcolon\coloneqq}{} & n && \left\{\fname{int\_of\_string}(n)\right\} \\
  \mathrel{|}{} & e_1\mathrel{\str{+}}e_2 && \left\{e_1 + e_2\right\} \\
  \mathrel{|}{} & \str{(}~e~\str{)} && \left\{e\right\}
\end{align*}
      \newtext[1]{Parser combinators can be easily extended to return a list not just of postfixes, but of pairs of a value and a postfix.  The \coqtype{parser} type can be parameterized over the type of the value returned.  The alternative combinator would return a disjoint union, and the sequencing combinator would return a pair of the two values returned by its inputs.  The terminal parser would return the single character it parsed, and the empty string parser would return an element of the singleton type.  Each rule for a nonterminal could then be wrapped with a combinator which applies the semantic action to the relevant values.  A more detailed explanation can be found in \cite{pcomb}.}   \newtext[2]{We describe in \autoref{sec:semantic-actions} how our parser can easily accommodate semantic actions.} 
      
    \subsubsection{Proving Correctness and Dealing with Nontermination}
      Although parser combinators are straightforward, it is easy to make them loop forever.  It is well-known that parsers defined naively using parser combinators don't handle grammars with \emph{left recursion}, where the first item in a given production rule is the nonterminal currently being defined.  For example, if we have the nonterminal \nt{expr} $\Coloneqq$ \nt{number}~|~\nt{expr}~\terminal{+}~\nt{expr}, then the parser for \nt{expr}~\terminal{+}~\nt{expr} will call the parser for \nt{expr}, which will call the parse for \nt{expr}~\terminal{+}~\nt{expr}, which will quickly loop forever.
      
      The algorithm we presented in \autoref{sec:solve-nontermination} is essentially the same as the algorithm Ridge presents in~\cite{Ridge} to deal with this problem.  By wrapping the calls to the parsers, in each combinator, with a function that prunes duplicative calls, Ridge provides a way to ensure that parsers terminate.  Also included in \cite{Ridge} are proofs in HOL4 that such wrapped parsers are both sound (and therefore terminating) and complete.  Furthermore, Ridge's parser has worst-case $O(n^5)$ running time in the input-string length.

  \subsection{Parsing with Derivatives}
    \Citeauthor{Derivs} describe an elegant method for recursive-descent parsing in~\cite{Derivs}, based on Brzozowski's derivatives~\cite{BrzozowskiDerivs}, which might be considered a conceptual dual to standard combinator parsing.  Rather than returning a list of possible string remnants, constructed by recursing down the structure of the grammar, we can iterate down the characters of a string, computing an updated language, or grammar, at each point.
    
    The \emph{language} defined by a grammar is the set of strings accepted by that grammar.  Here we describe the mathematical ideas behind parsing with derivatives.  \citeauthor*{Derivs} take a slightly different approach to ensure termination; where we will describe the mathematical operations on languages, they define these operations on a structural representation of the language, akin to an inductive definition of the grammar.
    
    Much as we defined parser combinators for the elementary operations of a grammar ($\epsilon$, terminals, sequencing, and alternatives), we can define similar combinators for defining a (lazy, or coinductive) language for a grammar.  Defining the type \fname{language} to be a set (or just a coinductive list) of strings, we have:
\begin{align*}
& \epsilon~:~\fname{language} \\
& \epsilon \functiondefeq \{\str{}\} \\ \\
& \fname{terminal}~:~\indname{Char}~\to~\fname{language} \\
& \fname{terminal}~\farg{ch} \functiondefeq \{\farg{ch}\} \\ \\
& (\fname{\sequencing})~:~\fname{language}~\to~\fname{language}~\to~\fname{language} \\
& \mathcal{L}_0~\fname{\sequencing}~\mathcal{L}_1 \functiondefeq \{~\strcat{\farg{s\ensuremath{_0}}}{\farg{s\ensuremath{_1}}}~|~\farg{s\ensuremath{_0}}\in\mathcal L_0\text{ and }\farg{s\ensuremath{_1}}\in\mathcal{L}_1 \} \\ \\
& (\fname{|||})~:~\fname{language}~\to~\fname{language}~\to~\fname{language} \\
& \mathcal{L}_0~\fname{|||}~\mathcal{L}_1 \functiondefeq \mathcal L_0\cup\mathcal L_1
\end{align*}

    The essential operations for computing derivatives are \emph{filtering} and \emph{chopping}.  To \emph{filter} a language $\mathcal L$ by a character $c$ is to take the subset of strings in $\mathcal L$ which start with $c$.  To chop a language $\mathcal L$ is to remove the first character from every string.  The derivative $D_c(\mathcal L)$ with respect to $c$ of a language $\mathcal L$ is then the language $\mathcal L$, filtered by $c$ and chopped:
\editedtext{\begin{align*}
&\fname{D}_{\farg{c}}~:~\fname{language}~\to~\fname{language} \\
&\editedtext{\fname{D}_{\farg{c}}~\ensuremath{\mathcal{L}} \functiondefeq }\bigcup_{\coqgroup{\cons{\farg{c}}{\farg{str}}}\in\mathcal L} \{\farg{str}\}
\end{align*}}%
      We can then define a \fname{has\_parse} proposition by taking successive derivatives:
\begin{align*}
&\fname{has\_parse}~:~\coqtype{language}~\to~\coqtype{String}~\to~\Prop \\
&\fname{has\_parse}~\mathcal{L}~\str{} \functiondefeq \str{}\in\mathcal{L} \\
&\fname{has\_parse}~\mathcal{L}~(\cons{\farg{ch}}{\farg{str}}) \functiondefeq \fname{has\_parse}~(\fname{D\ensuremath{_{\farg{ch}}}}~\mathcal L)~\farg{str}
\end{align*}
      To ensure termination and good performance, \citeauthor*{Derivs} define the derivative operation on the structure of the grammar, rather than defining combinators that turn a grammar into a language, and furthermore take advantage of laziness and memoization.  After adding code to prune the resulting language of useless content, they argue that the cost of parsing with derivatives is \editedtext{reasonable}.
      
    \subsubsection{Formal Verification}
      \Citeauthor{DerivsCoq} formally verify, in Coq, finite automata for parsing the fragment of derivative-based parsing which applies to regular expressions~\cite{DerivsCoq}.  This fragment dates back to Brzozowski's original presentation of derivatives~\cite{BrzozowskiDerivs}.
  
  \section{Other Approaches to Parsing}
    \newtext[2]{Recursive-descent parsing is not the only strategy for parsing.}%
    %\todo{Mention LR (and LL(1) and LALR) parsers: ``The field of parsing is one of the most venerable in computer-science.  Still with us are a variety of parsing approaches born in times of much more severe constraints on memory and processor speed, including various flavors of LR parsers, which apply only to strict subsets of the context-free grammars, to guarantee ability to predict which production applies based on finite look-ahead into a string.''  Adam also says these ``were considered `the only practical ones in the 20th century.'{}''}
    
    \paragraph{Top-Down Parsers: LL(\texorpdfstring{$k$}{k})}
      \newtext[2]{Recursive-descent parsing is a flavor of so-called ``top-down'' parsing; at each point in the algorithm, we know which nonterminal we are parsing the string as.  We thus build the parse tree from the top down, filling in more portions of the parse tree by picking which rule of a fixed nonterminal we should use.}
      
      \newtext[2]{Some context-free grammars have linear-time recursive-descent parsers that only require $k$ tokens after the current one being considered to decide which rule to apply; these grammars are called LL($k$) grammars.  More recently, arbitrary context-free grammars can be handled with Generalized LL parsers~\cite{GLL}, or with ALL(*) parsers~\cite{parr2014adaptive}, which are based on arbitrary look-ahead using regular expressions.}
      
    
    \paragraph{Bottom-Up Parsers: LR}
      \newtext[2]{Bottom-up parsers, of which LR parsers~\cite{chapman1987lr} are one of the most well-known flavors, instead associate the parts of the string which have already been parsed to complete parse trees.  For example, consider the grammar with two nonterminals, \nt{ab} $\Coloneqq$ \terminal{a}~\terminal{b}, and \nt{(ab)$^*$} $\Coloneqq$ $\epsilon$~\texttt{|}~\nt{ab}~\nt{(ab)$^*$}.  When parsing \str{abab} as \nt{(ab)$^*$}, an LR parser would parse \terminal{a} as the terminal \terminal{a}, parse \terminal{b} as the terminal \terminal{b}, and then reduce those two parse trees into a single parse tree for \nt{ab}.  It would then parse \terminal{a} as \terminal{a}, parse \terminal{b} as \terminal{b}, and then reduce those into the parse tree for \nt{ab}; we now have two parse trees for \nt{ab}.  Noticing that there are no characters remaining, the parser would reduce the latter \nt{ab} into a parse tree for \nt{(ab)$^*$} and then combine that with the earlier \nt{ab} parse tree to get a parse of the entire string as \nt{(ab)$^*$}.}
      
      \newtext[2]{LR parsers originated in the days when computers has much more stringent constraints on memory and processing power, and they apply only to strict subsets of context-free grammars; correctness and complexity guarantees rely on being able to uniquely determine what rule to apply based on a fixed lookahead.}
      
      \newtext[2]{More recently, Generalize LR (GLR) parsers have been devised which can handle all context-free grammars~\cite{tomita2013efficient}.}
          
    \paragraph{Parsing expression grammars (PEGs)} 
      \newtext[2]{\Citeauthor{PEG} proposes an alternative to context-free grammars, called parsing expression grammars~\cite{PEG}, which can always be deterministically parsed in linear time.  The basic idea is to incorporate some of the features of regular expressions directly into the grammar specification, and to drop the ability to have ambiguous alternatives; PEGs instead have prioritized alternatives.}
    
%    \todoask{Do I need to mention more kinds of parsers?  Do I need more explanation of these?  I feel like I did a pretty terrible job being comprehensive, but I don't think I have the mental stamina to keep reading about all the different ways of parsing for the purpose of ticking off checkboxes.}
    
  \section{Related Work on Verifying Parsers}    
%    \todoask{Is it okay to just use your paragraph here?  How much explanation is needed for each of these?}
    In addition to the work on verifying derivative-based parsing of regular expressions~\cite{DerivsCoq}, a few other past projects have verified parsers with proof assistants, applying to SLR~\cite{SLR} and LR(1)~\cite{LR1} parsers.  Several projects have used proof assistants to apply verified parsers within larger programming-language tools.  RockSalt~\cite{RockSalt} does run-time memory-safety enforcement for x86 binaries, relying on a verified machine-code parser that applies derivative-based parsing for regular expressions.  The verified Jitawa~\cite{Jitawa} and CakeML~\cite{CakeML} language implementations include verified parsers, handling Lisp and ML languages, respectively.
    
\section{What's New and What's Old} \label{sec:new} \label{sec:goals}
  The goal of this project is to demonstrate a new approach to generating parsers: incrementally building efficient parsers by refinement.
  
  We begin with naive recursive-descent parsing.  We ensure termination via memoization, a la~\cite{Ridge}.  We parameterize the parser on a ``splitting oracle'', which describes how to recurse (\autoref{sec:splitting-oracle}).  As far as we can tell, the idea of factoring the algorithmic complexity like this is new.
  
  We use the Coq library Fiat~\cite{delaware2015fiat} to incrementally build efficient parsers by refinement; we describe Fiat starting in \autoref{ch:fiat}.
  
  Additionally, we take a digression in \autoref{ch:dep-types} to describe how our parser can be used to prove its own completeness; the idea of reusing the parsing algorithm to generate proofs, parsing parse trees rather than strings, is not found in the literature, to the author's knowledge.
